{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Naive Bayes Classifier (NBC)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import re"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Probability Review\n",
    "### 1.1 Conditional Probability\n",
    "$$p(c|x) = \\frac{p(c,x)}{p(x)}$$\n",
    "### 1.2 Bayes' Theorem\n",
    "$$p(c|x) = \\frac{p(x|c)p(c)}{p(x)}$$\n",
    "### 1.3 Principle of Classification\n",
    "Denote $c_1$ as class 1, $c_2$ as class 2, $x$, $y$ are two independent features, if we have:\n",
    "$$\\begin{align*}\n",
    "p(c_1|x,y)&>p(c_2|x,y)\\\\\n",
    "\\frac{p(x,y|c_1)p(c_1)}{p(x,y)}&>\\frac{p(x,y|c_2)p(c_2)}{p(x,y)}\\text{,}\n",
    "\\end{align*}$$\n",
    "then we say the subject is more likely to be a member of $c_1$."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Text Classification\n",
    "### 2.1 Text Processing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_dataset():\n",
    "    postings=[['my','dog','has','flea','problems','help','please'],\\\n",
    "             ['maybe','not','take','him','to','dog','park','stupid'],\\\n",
    "             ['my','dalmation','is','so','cute','I','love','him'],\\\n",
    "             ['stop','posting','stupid','worthless','garbage'],\\\n",
    "             ['mr','licks','ate','my','steaks','how','to','stop','him'],\\\n",
    "             ['quit','buying','worthless','dog','food','stupid']]\n",
    "    labels = [0,1,0,1,0,1] #1 is insulting words, 0 is not\n",
    "    return postings, labels\n",
    "\n",
    "def create_vocab_list(dataset):\n",
    "    vocab_list = set([])\n",
    "    for record in dataset:\n",
    "        vocab_list = vocab_list|set(record)\n",
    "    return list(vocab_list)\n",
    "\n",
    "def record_to_vector(record, vocab_list):\n",
    "    vector = [0]*len(vocab_list)\n",
    "    for word in record:\n",
    "        if word in vocab_list:\n",
    "            vector[vocab_list.index(word)] += 1 \n",
    "            #each time of apperance is recorded, this model is called as bag-of-words-model\n",
    "            #another model is the set-of-words-model, which only cares about if one word appears or not\n",
    "            #vector[vocab_list.index(word)] = 1 \n",
    "        else:\n",
    "            print('The word %s is not in the vocabulary list.'%str(word))\n",
    "    return vector"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['stop', 'flea', 'maybe', 'ate', 'licks', 'help', 'quit', 'I', 'him', 'love', 'how', 'cute', 'has', 'not', 'stupid', 'my', 'garbage', 'food', 'please', 'posting', 'so', 'worthless', 'park', 'take', 'mr', 'to', 'dog', 'buying', 'is', 'dalmation', 'steaks', 'problems']\n",
      "[0, 1, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 1, 0, 0, 1, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 1]\n"
     ]
    }
   ],
   "source": [
    "postings, labels = create_dataset()\n",
    "v_list = create_vocab_list(postings)\n",
    "print(v_list)\n",
    "vector_0 = record_to_vector(postings[0], v_list)\n",
    "print(vector_0)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.2 Training\n",
    "Denote $w$ as a vector of a record, then we use:\n",
    "$$p(c_i|w)=\\frac{p(w|c_i)p(c_i)}{p(w)}$$\n",
    "to calculate the probability of belonging to class $i$.  \n",
    "If all features, *i.e.* $w_0,w_1,w_2,...$ are all indepent of each other, then we have:\n",
    "$$\\begin{align*}\n",
    "p(w|c_i) &= p(w_0,w_1,w_2,\\cdots|c_i)\\\\\n",
    "&= p(w_0|c_i)p(w_1|c_i)p(w_2|c_i)\\cdots p(w_N|c_i)\n",
    "\\end{align*}$$\n",
    "It is obvious that the denominator $p(w)$ is always a constant, so that it could be omitted during classification."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def nbc(vectorset, labels):\n",
    "    #vectorset here is collection of vectors generated by function record_to_vector\n",
    "    m = len(vectorset) #number of records\n",
    "    n = len(vectorset[0]) #number of features\n",
    "    p_class1 = np.sum(labels)/len(labels) #p of a record being an insult one\n",
    "    nump0 = np.zeros(n)\n",
    "    nump1 = np.zeros(n)\n",
    "    denomp0 = 0\n",
    "    denomp1 = 0\n",
    "    for i in range(m):\n",
    "        if labels[i] == 1: #if this is an insulting one\n",
    "            nump1 += vectorset[i] #numpy calculation, element-wise\n",
    "            denomp1 += sum(vectorset[i]) #+= number of words in this record\n",
    "        else:\n",
    "            nump0 += vectorset[i]\n",
    "            denomp0 += sum(vectorset[i])\n",
    "    p1_vector = nump1/denomp1 #p of each word in class 1\n",
    "    p0_vector = nump0/denomp0 #p of each word in class 2\n",
    "    print(denomp0,denomp1)\n",
    "    return p0_vector,p1_vector,p_class1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "24 19\n",
      "Probability of a record to be insulting:  0.5\n",
      "p0 vector:  [0.04166667 0.04166667 0.         0.04166667 0.04166667 0.04166667\n",
      " 0.         0.04166667 0.08333333 0.04166667 0.04166667 0.04166667\n",
      " 0.04166667 0.         0.         0.125      0.         0.\n",
      " 0.04166667 0.         0.04166667 0.         0.         0.\n",
      " 0.04166667 0.04166667 0.04166667 0.         0.04166667 0.04166667\n",
      " 0.04166667 0.04166667]\n",
      "p1 vector:  [0.05263158 0.         0.05263158 0.         0.         0.\n",
      " 0.05263158 0.         0.05263158 0.         0.         0.\n",
      " 0.         0.05263158 0.15789474 0.         0.05263158 0.05263158\n",
      " 0.         0.05263158 0.         0.10526316 0.05263158 0.05263158\n",
      " 0.         0.05263158 0.10526316 0.05263158 0.         0.\n",
      " 0.         0.        ]\n"
     ]
    }
   ],
   "source": [
    "postings, labels = create_dataset()\n",
    "v_list = create_vocab_list(postings)\n",
    "vectorset = []\n",
    "for record in postings:\n",
    "    vectorset.append(record_to_vector(record, v_list))\n",
    "vectorset = np.array(vectorset)\n",
    "p0_vector,p1_vector,p_class1 = nbc(vectorset, labels)\n",
    "print('Probability of a record to be insulting: ', p_class1)\n",
    "print('p0 vector: ', p0_vector)\n",
    "print('p1 vector: ', p1_vector)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.3 Modification\n",
    "When calculating $p(w|c_i) = p(w_0|c_i)p(w_1|c_i)p(w_2|c_i)\\cdots p(w_N|c_i)$, if any item equals to zero, then the result is also zero. To avoid this, during initialization, we set the appearance of each word as 1, and set denominator as 2.  \n",
    "Another problem is that if too mucn items are with small values, then the result will again becomes zero because of truncation. The solution is to use logrithm."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def modified_nbc(vectorset, labels):\n",
    "    m = len(vectorset) #number of records\n",
    "    n = len(vectorset[0]) #number of features\n",
    "    p_class1 = np.sum(labels)/len(labels) #p of a record being an insult one\n",
    "    nump0 = np.ones(n)\n",
    "    nump1 = np.ones(n)\n",
    "    denomp0 = 2\n",
    "    denomp1 = 2\n",
    "    for i in range(m):\n",
    "        if labels[i] == 1: #if this is an insulting one\n",
    "            nump1 = nump1 + vectorset[i] #numpy calculation, element-wise\n",
    "            denomp1 = denomp1 + sum(vectorset[i]) #+= number of words in this record\n",
    "        else:\n",
    "            nump0 = nump0 + vectorset[i]\n",
    "            denomp0 = denomp0 + sum(vectorset[i])\n",
    "    p1_vector = np.log(nump1/denomp1) #p of each word in class 1\n",
    "    p0_vector = np.log(nump0/denomp0) #p of each word in class 2\n",
    "    return p0_vector,p1_vector,p_class1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def classify(record_vector, p0_vector,p1_vector, p_class1):\n",
    "    p1 = np.sum(record_vector*p1_vector) + np.log(p_class1) #ln(a*b) = ln(a) + ln(b)\n",
    "    p0 = np.sum(record_vector*p0_vector) + np.log(1-p_class1)\n",
    "    if p1 > p0:\n",
    "        return 1\n",
    "    else:\n",
    "        return 0\n",
    "    \n",
    "def test_nbc(subject, dataset, labels):\n",
    "    vector_set = []\n",
    "    v_list = create_vocab_list(dataset)\n",
    "    for record in dataset:\n",
    "        vector = record_to_vector(record, v_list)\n",
    "        vector_set.append(vector)\n",
    "    vector_set = np.array(vector_set)\n",
    "    p0_vector, p1_vector, p_class1 = modified_nbc(vectorset, labels)\n",
    "    subject_vector = record_to_vector(subject, v_list)\n",
    "    label = classify(subject_vector, p0_vector, p1_vector, p_class1)\n",
    "    return label"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Class of ['love', 'my', 'dalmation'] is:  0\n",
      "Class of ['stupid', 'garbage'] is:  1\n"
     ]
    }
   ],
   "source": [
    "postings, labels = create_dataset()\n",
    "subj1 = 'love my dalmation'.split()\n",
    "subj2 = 'stupid garbage'.split()\n",
    "print('Class of ' + str(subj1) + ' is: ', test_nbc(subj1, postings, labels))\n",
    "print('Class of ' + str(subj2) + ' is: ', test_nbc(subj2, postings, labels))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Spam Filtering"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
